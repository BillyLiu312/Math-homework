











import torch
import torch.nn as nn
import numpy as np
import scipy.optimize
import matplotlib.pyplot as plt

def cycloid(xT, yT):
    "Return the exact parameters for the Brachistochrone curve (cycloid) from (0,0) to (xT, yT)"
    
    def y(theta):
        return yT/xT - (1-np.cos(theta))/(theta-np.sin(theta))
        
    thetaT = scipy.optimize.newton(y, np.pi/2) # solve for thetaT given (xT, yT)
    R = yT / (1 - np.cos(thetaT))
    
    return thetaT, R





class NN_Ansatz(nn.Module):
    "Defines the ansatz of Brachistochrone curve using a standard fully-connected network"
    "N_INPUT, N_OUTPUT = Dimension of Input, Output"
    "N_HIDDEN = Width of Hidden Layers"
    "N_LAYERS = Number of Hidden Layers"
    
    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
        super().__init__()
        activation = nn.Tanh

        # starting layer
        self.fcs = nn.Sequential(*[
                        nn.Linear(N_INPUT, N_HIDDEN),
                        activation()])

        # hidden layer
        self.fch = nn.Sequential(*[
                        nn.Sequential(*[
                            nn.Linear(N_HIDDEN, N_HIDDEN),
                            activation()]) for _ in range(N_LAYERS-1)])

        # ending layer
        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)
        
    def forward(self, x, xT, yT):
        
        # linear model
        # TODO: write model
        y_linear = x * (yT / xT)
        # residual model
        # TODO: write model
        y_residual = x * (x - xT) * self.fce(self.fch(self.fcs(x)))
        y = y_linear + y_residual
        
        return y





# define the problem
xT, yT = 1, 0.3
g = 9.81

# define our model
torch.manual_seed(0)
model = NN_Ansatz(1, 1, 8, 4)

# define number of step size for numercial integration
n = 10000
dx = (xT - 0) / n
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for it in range(10001):
    optimizer.zero_grad()

    # 1. numerical integration for computing the travel time 
    # generate grid points 
    # TODO: write code
    x = torch.linspace(0, xT, n, requires_grad=True).reshape((-1,1))
    # calculate the integrand 
    # TODO: write code
    y = model(x, xT, yT)
    y_safe = torch.clamp(y, min=1e-8)
    dy_dx = torch.autograd.grad(y_safe, x, grad_outputs=torch.ones_like(y_safe), create_graph=True)[0]
    integrand = torch.sqrt((1 + dy_dx**2) / (2 * g * y_safe))
    # trapezoid rule
    # TODO: write code
    T_NN = dx * (0.5 * integrand[0] + torch.sum(integrand[1:-1]) + 0.5 * integrand[-1])
    # 2. minimise the loss function to find Brachistochrone curve
    T_NN.backward()
    optimizer.step()
            
    # plot the results
    if it % 2000 == 0:
        
        x = torch.linspace(0,1,1000, requires_grad=True).reshape((-1,1))
        y = model(x, xT, yT)
        
        # get exact solution
        thetaT, R = cycloid(xT, yT)
        theta = np.linspace(0, thetaT, 1000)
        x_true = R * (theta - np.sin(theta))
        y_true = R * (1 - np.cos(theta))
        T_true = thetaT * np.sqrt(R / g)

        plt.figure(figsize=(12,5))
        plt.suptitle(f"Training step: {it}")
        
        plt.subplot(1,2,1)
        plt.title(f"Travel time: T_NN = {T_NN.item():.2f} s,  T_true = {T_true:.2f} s")            
        plt.plot(x_true, y_true, linewidth=2, label="Curve_true")
        plt.plot(x.detach()[:,0], y.detach()[:,0], linewidth=2, linestyle='dashed', label="Curve_NN")
        ylim = plt.ylim()
        plt.ylim(ylim[1], ylim[0])
        plt.xlabel("$x$"); plt.ylabel("$y$")
        plt.legend()
        
        plt.subplot(1,2,2)
        plt.title("Velocity")
        plt.plot(x.detach()[:,0], torch.sqrt(2*g*y).detach()[:,0], linewidth=2)
        plt.xlabel("$x$")
        plt.show()









def romberg_integration(f, a, b, max_level=17, eps=1e-6):
    """
    Romberg integration for PyTorch-compatible function f(x)
    f: callable, f(x) -> tensor of same shape as x
    a, b: scalars (integration limits)
    max_level: depth of Romberg table (default 4 => up to R[4,4])
    Returns: scalar tensor (integral value)
    """
    R = [[0.0 for _ in range(max_level + 1)] for _ in range(max_level + 1)]

    # Level 0: single trapezoid
    h = b - a
    x0 = torch.tensor([[a]], requires_grad=True)
    x1 = torch.tensor([[b]], requires_grad=True)
    R[0][0] = h * (f(x0) + f(x1)) / 2.0

    for k in range(1, max_level + 1):
        # Refine grid: 2^k intervals => 2^k + 1 points
        n_intervals = 2 ** k
        h = (b - a) / n_intervals
        x_mid = torch.linspace(a + h, b - h, n_intervals - 1).reshape(-1, 1)
        x_mid.requires_grad_(True)
        f_mid = f(x_mid).sum()
        f_end = f(torch.tensor([[a]], requires_grad=True)) + f(torch.tensor([[b]], requires_grad=True))
        R[k][0] = h * (f_end / 2.0 + f_mid)

        # Richardson extrapolation
        for m in range(1, k + 1):
            factor = 1.0 / (4 ** m - 1)
            R[k][m] = R[k][m - 1] + factor * (R[k][m - 1] - R[k - 1][m - 1])

        # Optional early stopping if converged
        if k > 1 and abs(R[k][k] - R[k - 1][k - 1]) < eps:
            return R[k][k]

    return R[max_level][max_level]


# Reuse your model and problem setup
xT, yT = 1.0, 0.3
g = 9.81
torch.manual_seed(0)
model = NN_Ansatz(1, 1, 32, 4)  # slightly wider for better expressivity
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# Avoid singularity at x=0
eps = 1e-6

def integrand_func(x):
    """Closure for integrand: must be differentiable w.r.t. model params"""
    y = model(x, xT, yT)
    dy_dx = torch.autograd.grad(
        y, x,
        grad_outputs=torch.ones_like(y),
        create_graph=True
    )[0]
    y_safe = torch.clamp(y, min=1e-8)
    return torch.sqrt((1 + dy_dx**2) / (2 * g * y_safe))

for it in range(10001):
    optimizer.zero_grad()

    # Compute travel time using Romberg integration
    T_NN = romberg_integration(integrand_func, eps, xT, max_level=4)

    T_NN.backward()
    optimizer.step()

    if it % 2000 == 0:
        with torch.no_grad():
            x_plot = torch.linspace(0, xT, 500).reshape(-1, 1)
            y_plot = model(x_plot, xT, yT)

            thetaT, R = cycloid(xT, yT)
            theta = np.linspace(0, thetaT, 500)
            x_true = R * (theta - np.sin(theta))
            y_true = R * (1 - np.cos(theta))
            T_true = thetaT * np.sqrt(R / g)

        print(f"Iter {it}: T_NN = {T_NN.item():.6f}, T_true = {T_true:.6f}, error = {abs(T_NN.item() - T_true):.2e}")

        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(x_true, y_true, 'k-', label='Exact')
        plt.plot(x_plot[:,0], y_plot[:,0], 'r--', label='PINN (Romberg)')
        plt.ylim(plt.ylim()[::-1])
        plt.xlabel('x'); plt.ylabel('y'); plt.legend(); plt.title('Brachistochrone Curve')

        plt.subplot(1, 2, 2)
        plt.plot(x_plot[:,0], torch.sqrt(2*g*y_plot)[:,0], 'b-')
        plt.xlabel('x'); plt.ylabel('Velocity'); plt.title('Velocity Profile')
        plt.tight_layout()
        plt.show()














def cycloid_with_friction(xT, yT, mu):
    "Return the exact parameters for the Brachistochrone curve (cycloid) from (0,0) to (xT, yT)"
    
    def y(theta):
        return yT/xT - (1 - np.cos(theta) + mu * (theta + np.sin(theta)))/(theta - np.sin(theta) + mu * (1 - np.cos(theta)))
        
    thetaT = scipy.optimize.newton(y, np.pi/2) # solve for thetaT given (xT, yT)
    R = yT / (1 - np.cos(thetaT) + mu * (thetaT + np.sin(thetaT)))
    
    return thetaT, R





from scipy.integrate import quad
xT, yT = 1.0, 0.3
g = 9.81    
mu = 0.1  # friction coefficient
thetaT, R = cycloid_with_friction(xT, yT, mu)
theta = np.linspace(0, thetaT, 500)
x_true = R * (theta - np.sin(theta) + mu * (1 - np.cos(theta)))
y_true = R * (1 - np.cos(theta) + mu * (theta + np.sin(theta)))
T_true = thetaT * np.sqrt(R / g)

def integrand(theta):
    # Denominator inside sqrt: D(θ)/R = (1 - μ²)(1 - cosθ) + 2μ sinθ
    term = (1 - mu**2) * (1 - np.cos(theta)) + 2 * mu * np.sin(theta)
    if term <= 0:
        # Should not happen for valid theta in [0, thetaT]
        return np.inf
    return 1.0 / np.sqrt(term)

integral_val, _ = quad(integrand, 0, thetaT, epsabs=1e-10, epsrel=1e-10)
T = np.sqrt(R / (2 * g)) * integral_val

# Travel time with friction
def compute_travel_time_with_friction(model, xT, yT, g, mu, n=8192):
    eps = 1e-6
    x = torch.linspace(eps, xT, n, requires_grad=True).reshape(-1, 1)
    y = model(x, xT, yT)
    
    # Compute dy/dx
    dy_dx = torch.autograd.grad(
        y, x,
        grad_outputs=torch.ones_like(y),
        create_graph=True
    )[0]
    
    # Friction-aware denominator: y - mu * x
    denom = y - mu * x
    denom_safe = torch.clamp(denom, min=1e-8)  # critical!
    
    integrand = torch.sqrt((1 + dy_dx**2) / (2 * g * denom_safe))
    
    dx = (xT - eps) / (n - 1)
    T = dx * (0.5 * integrand[0] + integrand[1:-1].sum() + 0.5 * integrand[-1])
    return T

# ----------------------------
# Training
# ----------------------------
torch.manual_seed(42)
model = NN_Ansatz(N_INPUT=1, N_HIDDEN=64, N_LAYERS=5, N_OUTPUT=1)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

print(f"Friction coefficient μ = {mu}")
for it in range(10001):
    optimizer.zero_grad()
    T_NN = compute_travel_time_with_friction(model, xT, yT, g, mu, n=8192)
    T_NN.backward()
    optimizer.step()

    if it % 2000 == 0:
        with torch.no_grad():
            x_plot = torch.linspace(0, xT, 500).reshape(-1, 1)
            y_plot = model(x_plot, xT, yT)

        print(f"Iter {it}: T_NN = {T_NN.item():.6f}, T_true = {T_true:.6f}, error = {abs(T_NN.item() - T_true):.2e}")

        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(x_true, y_true, 'k-', label='Exact')
        plt.plot(x_plot[:,0], y_plot[:,0], 'r--', label='PINN (Romberg)')
        plt.ylim(plt.ylim()[::-1])
        plt.xlabel('x'); plt.ylabel('y'); plt.legend(); plt.title('Brachistochrone Curve')

        plt.subplot(1, 2, 2)
        plt.plot(x_plot[:,0], torch.sqrt(2*g*y_plot)[:,0], 'b-')
        plt.xlabel('x'); plt.ylabel('Velocity'); plt.title('Velocity Profile')
        plt.tight_layout()
        plt.show()











x1, y1 = 0.0, 0.0
x2, y2 = 1.0, 0.3
L = 2.0  # length of the cable
rho = 1.0  # density
g = 9.81  # gravity

def catenary(x1, y1, x2, y2):
    "Return the exact parameters for the Catenary curve from (x1,y1) to (x2,y2)"
    
    def equations(p):
        a, c, x0 = p
        eq1 = c * np.cosh((x1 - x0) / c) - a - y1
        eq2 = c * np.cosh((x2 - x0) / c) - a - y2
        eq3 = c * (np.sinh((x2 - x0) / c) - np.sinh((x1 - x0) / c)) - L
        return (eq1, eq2, eq3)
        
    a_initial = 1.0
    c_initial = (x1 + x2) / 2
    x0_initial = x1 + (x2 - x1) / 2
    a, c, x0 = scipy.optimize.fsolve(equations, (a_initial, c_initial, x0_initial))
    
    return a, c, x0

a, c, x0 = catenary(x1, y1, x2, y2)
x_true = np.linspace(x1, x2, 500)
y_true = c * np.cosh((x_true - x0) / c) - a
def integrand(x):
    y = c * np.cosh((x - x0) / c) - a
    return y**2 + a * y

integral_val, _ = quad(integrand, x1, x2, epsabs=1e-10, epsrel=1e-10)
U = rho * g / c * integral_val
print(f"Exact potential energy U = {U:.6f} J")

plt.figure(figsize=(6,4))
plt.plot(x_true, y_true, 'k-', label='Exact Catenary')
plt.xlabel('x'); plt.ylabel('y'); plt.legend(); plt.title('Catenary Curve')
plt.show()





class NN_Ansatz2(nn.Module):
    "Defines the ansatz of Brachistochrone curve using a standard fully-connected network"
    "N_INPUT, N_OUTPUT = Dimension of Input, Output"
    "N_HIDDEN = Width of Hidden Layers"
    "N_LAYERS = Number of Hidden Layers"
    
    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):
        super().__init__()
        activation = nn.Tanh

        # starting layer
        self.fcs = nn.Sequential(*[
                        nn.Linear(N_INPUT, N_HIDDEN),
                        activation()])

        # hidden layer
        self.fch = nn.Sequential(*[
                        nn.Sequential(*[
                            nn.Linear(N_HIDDEN, N_HIDDEN),
                            activation()]) for _ in range(N_LAYERS-1)])

        # ending layer
        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)
        
    def forward(self, x, x0, y0, x1, y1):
        
        # linear model
        # TODO: write model
        y_linear = (x - x0) * (y1 - y0) / (x1 - x0) + y0
        # residual model
        # TODO: write model
        y_residual = (x - x0) * (x - x1) * torch.exp(self.fce(self.fch(self.fcs(x))))
        y = y_linear + y_residual
        
        return y


# punishment method
# define our model
torch.manual_seed(0)
model = NN_Ansatz2(1, 1, 8, 4)

# define number of step size for numercial integration
n = 10000
dx = (xT - 0) / n
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
alpha = 1e5  # 长度约束权重，可调

for it in range(10001):
    optimizer.zero_grad()

    # 1. numerical integration for computing the travel time 
    # generate grid points 
    # TODO: write code
    x = torch.linspace(x0, x1, n, requires_grad=True).reshape((-1,1))
    # calculate the integrand 
    # TODO: write code
    y = model(x, x1, y1, x2, y2)
    dy_dx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    ds = torch.sqrt(1 + dy_dx**2)
    potential_density = y * ds
    # trapezoid rule
    # TODO: write code
    U_NN = dx * (0.5 * potential_density[0] + torch.sum(potential_density[1:-1]) + 0.5 * potential_density[-1])
    L_pred = dx * (0.5 * ds[0] + torch.sum(ds[1:-1]) + 0.5 * ds[-1])
    # 2. minimise the loss function to find Brachistochrone curve
    loss = U_NN + alpha * (L_pred - L)**2
    loss.backward()
    optimizer.step()
            
    # plot the results
    if it % 2000 == 0:
        
        x = torch.linspace(0,1,1000, requires_grad=True).reshape((-1,1))
        y = model(x, x1, y1, x2, y2)

        plt.figure(figsize=(12,5))
        plt.suptitle(f"Training step: {it}")
        
        plt.subplot(1,2,1)
        plt.title(f"Travel time: U_NN = {rho * g * U_NN.item():.2f} J,  U_true = {U:.2f} J,  L Diff = {(L_pred.item() - L):.2f} m")            
        plt.plot(x_true, y_true, linewidth=2, label="Curve_true")
        plt.plot(x.detach()[:,0], y.detach()[:,0], linewidth=2, linestyle='dashed', label="Curve_NN")
        plt.xlabel("$x$"); plt.ylabel("$y$")
        plt.legend()
        plt.show()



# 定义可学习的拉格朗日乘子
lambda_param = nn.Parameter(torch.tensor(0.0))
model2 = NN_Ansatz2(1, 1, 8, 4)

def loss_fn(model):
    x = torch.linspace(x1, x2, n, requires_grad=True).reshape(-1, 1)
    y = model(x, x1, y1, x2, y2)
    dy_dx = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True)[0]
    
    # 计算势能和长度
    ds = torch.sqrt(1 + dy_dx**2)
    U = torch.trapz(y.squeeze() * ds.squeeze(), x.squeeze())
    L_pred = torch.trapz(ds.squeeze(), x.squeeze())
    
    # 拉格朗日损失
    loss = U + lambda_param * (L_pred - L)
    return loss, U.item(), L_pred.item()

# 优化所有参数（包括 lambda）
optimizer = torch.optim.Adam(list(model2.parameters()) + [lambda_param], lr=1e-3)

for it in range(10001):
    optimizer.zero_grad()
    loss, U_val, L_val = loss_fn(model2)
    loss.backward()
    optimizer.step()

    # plot the results
    if it % 2000 == 0:
        
        x = torch.linspace(0,1,1000, requires_grad=True).reshape((-1,1))
        y = model(x, x1, y1, x2, y2)

        plt.figure(figsize=(12,5))
        plt.suptitle(f"Training step: {it}")
        
        plt.subplot(1,2,1)
        plt.title(f"Travel time: U_NN = {rho * g * U_NN.item():.2f} J,  U_true = {U:.2f} J")            
        plt.plot(x_true, y_true, linewidth=2, label="Curve_true")
        plt.plot(x.detach()[:,0], y.detach()[:,0], linewidth=2, linestyle='dashed', label="Curve_NN")
        plt.xlabel("$x$"); plt.ylabel("$y$")
        plt.legend()
        plt.show()
